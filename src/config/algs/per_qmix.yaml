# --- DIFFER specific parameters ---
#基于qmix的differ在smac运行情况良好，但是在GRF上可能不佳

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

runner: "episode"

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
learner: "per_learner" #super
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

#使用专用schema
scheme: "differ"

#独有参数
selected: "PER_weight"
warm_up: True
selected_alpha: 0.8  #计算优先级分数用的参数
selected_ratio: 0.6  #采样比例
selected_ratio_start: 0.8  #PER采样比例的起始值
selected_ratio_end: 1.0  #PER采样比例的最终值
warm_up_ratio: 0.6  #热身区间占总训练轮次的比例。在热身区间，selected_radio会从start线性增长到end
selected_epsilon: 0.01  #采样可能性的参数
beta_start: 0.6  #PER优先级采样中，beta的起始值
beta_end: 1  #PER优先级采样中，beta的最终值

#测试参数
use_ESR_based_on_normal_distribution: True #使用基于正态分布的经验选择与接收算法

name: "per_qmix"